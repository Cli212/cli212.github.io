<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Mind The Gap! Overcoming The Training-inference Divide of Masked Diffusion Language Models</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
:root {
  --color-scheme: only light;
  --bg: #fff;
  --fg: #111;
  --border: #ccc;
  --note: #666;
  --control-bg: #f5f5f5;
  --control-fg: #111;
  /* Extras for this stylesheet */
  --fg-muted: rgba(55, 53, 47, 0.6);
  --border-soft: rgba(55, 53, 47, 0.09);
  --bg-subtle: #f7f6f3; /* light table/header tint */
}

/* Dark theme overrides */
/*@media (prefers-color-scheme: light) {*/
/*  :root {*/
/*    --bg: #121212;*/
/*    --fg: #eee;*/
/*    --border: #333;*/
/*    --note: #aaa;*/
/*    --control-bg: #1e1e1e;*/
/*    --control-fg: #eee;*/

/*    --fg-muted: rgba(238, 238, 238, 0.6);*/
/*    --border-soft: rgba(238, 238, 238, 0.12);*/
/*    --bg-subtle: #1f1f1f; !* darker subtle tint *!*/
/*  }*/
/*}*/

/* ===========================
   BASE / PRINT
   =========================== */

/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
  -webkit-print-color-adjust: exact;
}
* {
  box-sizing: border-box;
  -webkit-print-color-adjust: exact;
}

html,
body {
  margin: 0;
  padding: 0;
  background-color: var(--bg);
  color: var(--fg);
}

/* Screen layout wrapper */
@media only screen {
  body {
    margin: 2em auto;
    max-width: 900px;
    color: var(--fg);
  }
}

body {
  line-height: 1.5;
  white-space: pre-wrap;
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
}

a,
a.visited {
  color: inherit;
  text-decoration: underline;
}

.pdf-relative-link-path {
  font-size: 80%;
  color: var(--note);
}

h1,
h2,
h3 {
  letter-spacing: -0.01em;
  line-height: 1.2;
  font-weight: 600;
  margin-bottom: 0;
  color: var(--fg);
}

.page-title {
  font-size: 2.5rem;
  font-weight: 700;
  margin-top: 0;
  margin-bottom: 0.75em;
  color: var(--fg);
}

h1 {
  font-size: 1.875rem;
  margin-top: 1.875rem;
}
h2 {
  font-size: 1.5rem;
  margin-top: 1.5rem;
}
h3 {
  font-size: 1.25rem;
  margin-top: 1.25rem;
}

.source {
  border: 1px solid var(--border);
  border-radius: 3px;
  padding: 1.5em;
  word-break: break-all;
  background: var(--bg);
}

.callout {
  border-radius: 3px;
  padding: 1rem;
  background: var(--bg-subtle);
}

figure {
  margin: 1.25em 0;
  page-break-inside: avoid;
}
figcaption {
  opacity: 0.8;
  color: var(--fg-muted);
  font-size: 85%;
  margin-top: 0.5em;
}

mark {
  background-color: transparent;
}

.indented {
  padding-left: 1.5em;
}

hr {
  background: transparent;
  display: block;
  width: 100%;
  height: 1px;
  visibility: visible;
  border: none;
  border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
  max-width: 100%;
}

@media only print {
  img {
    max-height: 100vh;
    object-fit: contain;
  }
}

@page {
  margin: 1in;
}

/* ===========================
   CONTROLS (from your theme)
   =========================== */

label, select, input, button {
  font-size: 1rem;
  margin: 0.5em 0;
  color: var(--control-fg);
}

select, input {
  width: 100%;
  padding: 0.5em;
  box-sizing: border-box;
  background-color: var(--control-bg);
  border: 1px solid var(--border);
  color: var(--control-fg);
}

button {
  padding: 0.5em 1em;
  cursor: pointer;
  background-color: var(--control-bg);
  border: 1px solid var(--border);
  color: var(--control-fg);
}

.note {
  color: var(--note);
  font-size: 0.9rem;
}

/* Optional demo frame hook (kept from your theme) */
#demo-frame {
  display: none;
  width: 100%;
  height: 600px;
  border: 1px solid var(--border);
  margin-top: 1em;
  background-color: var(--bg);
}

/* ===========================
   LAYOUT / TYPOGRAPHY
   =========================== */

.collection-content {
  font-size: 0.875rem;
}

.column-list {
  display: flex;
  justify-content: space-between;
}

.column {
  padding: 0 1em;
}
.column:first-child { padding-left: 0; }
.column:last-child { padding-right: 0; }

.table_of_contents-item {
  display: block;
  font-size: 0.875rem;
  line-height: 1.3;
  padding: 0.125rem;
}
.table_of_contents-indent-1 { margin-left: 1.5rem; }
.table_of_contents-indent-2 { margin-left: 3rem; }
.table_of_contents-indent-3 { margin-left: 4.5rem; }

.table_of_contents-link {
  text-decoration: none;
  opacity: 0.7;
  border-bottom: 1px solid var(--border-soft);
  color: var(--fg);
}

table,
th,
td {
  border: 1px solid var(--border-soft);
  border-collapse: collapse;
}

table {
  border-left: none;
  border-right: none;
  background: var(--bg);
}

th,
td {
  font-weight: normal;
  padding: 0.25em 0.5em;
  line-height: 1.5;
  min-height: 1.5em;
  text-align: left;
  color: var(--fg);
}

th {
  color: var(--fg-muted);
}

ol,
ul {
  margin: 0;
  margin-block-start: 0.6em;
  margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
  margin-block-start: 0.6em;
}

ul > li { list-style: disc; }

ul.to-do-list { padding-inline-start: 0; }
ul.to-do-list > li { list-style: none; }

.to-do-children-checked {
  text-decoration: line-through;
  opacity: 0.375;
}

ul.toggle > li { list-style: none; }

ul { padding-inline-start: 1.7em; }
ul > li { padding-left: 0.1em; }

ol { padding-inline-start: 1.6em; }
ol > li { padding-left: 0.2em; }

.mono ol { padding-inline-start: 2em; }
.mono ol > li { text-indent: -0.4em; }

.toggle {
  padding-inline-start: 0em;
  list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
  padding-left: 1.7em;
}
.toggle > li > details > summary {
  margin-left: -1.1em;
}

.selected-value {
  display: inline-block;
  padding: 0 0.5em;
  background: rgba(206, 205, 202, 0.5);
  border-radius: 3px;
  margin-right: 0.5em;
  margin-top: 0.3em;
  margin-bottom: 0.3em;
  white-space: nowrap;
  color: var(--fg);
}

.collection-title {
  display: inline-block;
  margin-right: 1em;
}

.page-description {
  margin-bottom: 2em;
  color: var(--fg-muted);
}

.simple-table {
  margin-top: 1em;
  font-size: 0.875rem;
  empty-cells: show;
  background: var(--bg);
}
.simple-table td,
.simple-table th {
  height: 29px;
  min-width: 120px;
}

.simple-table-header-color {
  background: var(--bg-subtle);
  color: var(--fg);
}
.simple-table-header {
  font-weight: 500;
}

time { opacity: 0.8; color: var(--fg-muted); }

.icon {
  display: inline-block;
  max-width: 1.2em;
  max-height: 1.2em;
  text-decoration: none;
  vertical-align: text-bottom;
  margin-right: 0.5em;
}

img.icon { border-radius: 3px; }

.user-icon {
  width: 1.5em;
  height: 1.5em;
  border-radius: 100%;
  margin-right: 0.5rem;
}
.user-icon-inner { font-size: 0.8em; }

.text-icon {
  border: 1px solid var(--fg);
  text-align: center;
}

.page-cover-image {
  display: block;
  object-fit: cover;
  width: 100%;
  max-height: 30vh;
}

.page-header-icon {
  font-size: 3rem;
  margin-bottom: 1rem;
  color: var(--fg);
}

.page-header-icon-with-cover {
  margin-top: -0.72em;
  margin-left: 0.07em;
}

.page-header-icon img { border-radius: 3px; }

.link-to-page {
  margin: 1em 0;
  padding: 0;
  border: none;
  font-weight: 500;
  color: var(--fg);
}

p > .user { opacity: 0.8; color: var(--fg-muted); }

td > .user,
td > time {
  white-space: nowrap;
}

input[type="checkbox"] {
  transform: scale(1.5);
  margin-right: 0.6em;
  vertical-align: middle;
}

/* Paragraphs */
p {
  margin-top: 0.5em;
  margin-bottom: 0.5em;
  color: var(--fg);
}

.image {
  border: none;
  margin: 1.5em 0;
  padding: 0;
  border-radius: 0;
  text-align: center;
  background: transparent;
}

/* Code styling (kept mostly as-is for syntax contrast) */
.code,
code {
  background: rgba(135, 131, 120, 0.15);
  border-radius: 3px;
  padding: 0.2em 0.4em;
  font-size: 85%;
  tab-size: 2;
}

code { color: #eb5757; }

.code { padding: 1.5em 1em; }
.code-wrap { white-space: pre-wrap; word-break: break-all; }
.code > code {
  background: none;
  padding: 0;
  font-size: 100%;
  color: inherit;
}

/* Blockquote uses theme fg for the rule */
blockquote {
  font-size: 1.25em;
  margin: 1em 0;
  padding-left: 1em;
  border-left: 3px solid var(--fg);
  color: var(--fg);
}

/* Bookmark card */
.bookmark {
  text-decoration: none;
  max-height: 8em;
  padding: 0;
  display: flex;
  width: 100%;
  align-items: stretch;
  border: 1px solid var(--border-soft);
  background: var(--bg);
}
.bookmark-title {
  font-size: 0.85em;
  overflow: hidden;
  text-overflow: ellipsis;
  height: 1.75em;
  white-space: nowrap;
  color: var(--fg);
}
.bookmark-text {
  display: flex;
  flex-direction: column;
}
.bookmark-info {
  flex: 4 1 180px;
  padding: 12px 14px 14px;
  display: flex;
  flex-direction: column;
  justify-content: space-between;
}
.bookmark-image {
  width: 33%;
  flex: 1 1 180px;
  display: block;
  position: relative;
  object-fit: cover;
  border-radius: 1px;
}
.bookmark-description {
  color: var(--fg-muted);
  font-size: 0.75em;
  overflow: hidden;
  max-height: 4.5em;
  word-break: break-word;
}
.bookmark-href {
  font-size: 0.75em;
  margin-top: 0.25em;
  color: var(--note);
}

/* ===========================
   FONTS (kept as provided)
   =========================== */

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }

/* ===========================
   COLOR UTILS (kept as-is)
   =========================== */

.highlight-default { color: rgba(50, 48, 44, 1); }
.highlight-gray { color: rgba(115, 114, 110, 1); fill: rgba(115, 114, 110, 1); }
.highlight-brown { color: rgba(159, 107, 83, 1); fill: rgba(159, 107, 83, 1); }
.highlight-orange { color: rgba(217, 115, 13, 1); fill: rgba(217, 115, 13, 1); }
.highlight-yellow { color: rgba(203, 145, 47, 1); fill: rgba(203, 145, 47, 1); }
.highlight-teal { color: rgba(68, 131, 97, 1); fill: rgba(68, 131, 97, 1); }
.highlight-blue { color: rgba(51, 126, 169, 1); fill: rgba(51, 126, 169, 1); }
.highlight-purple { color: rgba(144, 101, 176, 1); fill: rgba(144, 101, 176, 1); }
.highlight-pink { color: rgba(193, 76, 138, 1); fill: rgba(193, 76, 138, 1); }
.highlight-red { color: rgba(205, 60, 58, 1); fill: rgba(205, 60, 58, 1); }

.highlight-default_background { color: rgba(50, 48, 44, 1); }
.highlight-gray_background { background: rgba(248, 248, 247, 1); }
.highlight-brown_background { background: rgba(244, 238, 238, 1); }
.highlight-orange_background { background: rgba(251, 236, 221, 1); }
.highlight-yellow_background { background: rgba(251, 243, 219, 1); }
.highlight-teal_background { background: rgba(237, 243, 236, 1); }
.highlight-blue_background { background: rgba(231, 243, 248, 1); }
.highlight-purple_background { background: rgba(248, 243, 252, 1); }
.highlight-pink_background { background: rgba(252, 241, 246, 1); }
.highlight-red_background { background: rgba(253, 235, 236, 1); }

.block-color-default { color: inherit; fill: inherit; }
.block-color-gray { color: rgba(115, 114, 110, 1); fill: rgba(115, 114, 110, 1); }
.block-color-brown { color: rgba(159, 107, 83, 1); fill: rgba(159, 107, 83, 1); }
.block-color-orange { color: rgba(217, 115, 13, 1); fill: rgba(217, 115, 13, 1); }
.block-color-yellow { color: rgba(203, 145, 47, 1); fill: rgba(203, 145, 47, 1); }
.block-color-teal { color: rgba(68, 131, 97, 1); fill: rgba(68, 131, 97, 1); }
.block-color-blue { color: rgba(51, 126, 169, 1); fill: rgba(51, 126, 169, 1); }
.block-color-purple { color: rgba(144, 101, 176, 1); fill: rgba(144, 101, 176, 1); }
.block-color-pink { color: rgba(193, 76, 138, 1); fill: rgba(193, 76, 138, 1); }
.block-color-red { color: rgba(205, 60, 58, 1); fill: rgba(205, 60, 58, 1); }

.block-color-default_background { color: inherit; fill: inherit; }
.block-color-gray_background { background: rgba(248, 248, 247, 1); }
.block-color-brown_background { background: rgba(244, 238, 238, 1); }
.block-color-orange_background { background: rgba(251, 236, 221, 1); }
.block-color-yellow_background { background: rgba(251, 243, 219, 1); }
.block-color-teal_background { background: rgba(237, 243, 236, 1); }
.block-color-blue_background { background: rgba(231, 243, 248, 1); }
.block-color-purple_background { background: rgba(248, 243, 252, 1); }
.block-color-pink_background { background: rgba(252, 241, 246, 1); }
.block-color-red_background { background: rgba(253, 235, 236, 1); }

/* ===========================
   SELECT/TOGGLE/ICONS
   =========================== */

.select-value-color-default { background-color: rgba(84, 72, 49, 0.08); }
.select-value-color-gray { background-color: rgba(84, 72, 49, 0.15); }
.select-value-color-brown { background-color: rgba(210, 162, 141, 0.35); }
.select-value-color-orange { background-color: rgba(224, 124, 57, 0.27); }
.select-value-color-yellow { background-color: rgba(236, 191, 66, 0.39); }
.select-value-color-green { background-color: rgba(123, 183, 129, 0.27); }
.select-value-color-blue { background-color: rgba(93, 165, 206, 0.27); }
.select-value-color-purple { background-color: rgba(168, 129, 197, 0.27); }
.select-value-color-pink { background-color: rgba(225, 136, 179, 0.27); }
.select-value-color-red { background-color: rgba(244, 171, 159, 0.4); }

.checkbox {
  display: inline-flex;
  vertical-align: text-bottom;
  width: 16;
  height: 16;
  background-size: 16px;
  margin-left: 2px;
  margin-right: 5px;
}

.checkbox-on {
  background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
  background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="1edf34ea-c0ab-8009-945d-d2775a19c4ac" class="page sans"><header><img class="page-cover-image" src="https://www.notion.so/images/page-cover/webb4.jpg" style="object-position:center 50%"/><div class="page-header-icon page-header-icon-with-cover"><span class="icon">üåÄ</span></div><h1 class="page-title">Mind The Gap! Overcoming The Training-inference Divide of Masked Diffusion Language Models</h1><p class="page-description"></p><table class="properties"><tbody></tbody></table></header><div class="page-body"><p id="1edf34ea-c0ab-81aa-a22d-e9bd280ce837" class=""><a href="https://cli212.github.io/">Haoyu He</a>, <a href="https://www.katrinrenz.de/">Katrin Renz</a>, <a href="https://yongcaoplus.github.io/">Yong Cao</a>, <a href="https://www.cvlibs.net/">Andreas Geiger</a></p><p id="1edf34ea-c0ab-81e9-b0b4-f9d439b85a53" class="">                                                                                                                                            18 Aug, 2025</p>
  <p id="253f34ea-c0ab-8042-b975-f2e8bbb2b7ac" class="" style="display: flex; align-items: center; gap: 24px; font-size: 16px;">
    <a href="https://arxiv.org/pdf/2508.13148" style="display: flex; align-items: center; color: #0066cc; text-decoration: none;">
        <svg style="width: 20px; height: 20px; margin-right: 6px; flex-shrink: 0;" viewBox="0 0 24 24" fill="currentColor">
            <path d="M14,2H6A2,2 0 0,0 4,4V20A2,2 0 0,0 6,22H18A2,2 0 0,0 20,20V8L14,2M18,20H6V4H13V9H18V20Z" />
        </svg>
        <span style="font-weight: 500;">Paper</span>
    </a>
    <a href="https://github.com/autonomousvision/mdpo" style="display: flex; align-items: center; color: #0066cc; text-decoration: none;">
        <svg style="width: 20px; height: 20px; margin-right: 6px; flex-shrink: 0;" viewBox="0 0 24 24" fill="currentColor">
            <path d="M12,2A10,10 0 0,0 2,12C2,16.42 4.87,20.17 8.84,21.5C9.34,21.58 9.5,21.27 9.5,21C9.5,20.77 9.5,20.14 9.5,19.31C6.73,19.91 6.14,17.97 6.14,17.97C5.68,16.81 5.03,16.5 5.03,16.5C4.12,15.88 5.1,15.9 5.1,15.9C6.1,15.97 6.63,16.93 6.63,16.93C7.5,18.45 8.97,18 9.54,17.76C9.63,17.11 9.89,16.67 10.17,16.42C7.95,16.17 5.62,15.31 5.62,11.5C5.62,10.39 6,9.5 6.65,8.79C6.55,8.54 6.2,7.5 6.75,6.15C6.75,6.15 7.59,5.88 9.5,7.17C10.29,6.95 11.15,6.84 12,6.84C12.85,6.84 13.71,6.95 14.5,7.17C16.41,5.88 17.25,6.15 17.25,6.15C17.8,7.5 17.45,8.54 17.35,8.79C18,9.5 18.38,10.39 18.38,11.5C18.38,15.32 16.04,16.16 13.81,16.41C14.17,16.72 14.5,17.33 14.5,18.26C14.5,19.6 14.5,20.68 14.5,21C14.5,21.27 14.66,21.59 15.17,21.5C19.14,20.16 22,16.42 22,12A10,10 0 0,0 12,2Z" />
        </svg>
        <span style="font-weight: 500;">GitHub</span>
    </a>
</p>
  <hr id="1edf34ea-c0ab-8120-bfe1-ded0c6f8204e"/><p id="253f34ea-c0ab-80e3-8d03-ca2a9d29c7a8" class="">
</p><p id="1edf34ea-c0ab-8128-b60c-ddec6de3e443" class="">Masked Diffusion Language Models (MDLMs), as a promising alternative to autoregressive (AR) LLMs, enable bidirectional conditioning, faster generation, and parallel decoding</p><p id="253f34ea-c0ab-804d-b5be-cee80d8d1c9b" class="">However current MDLMs face two fundamental problems:</p><ul id="253f34ea-c0ab-8048-bc70-cc7f7c9cd1e9" class="bulleted-list"><li style="list-style-type:disc"><strong>Training‚ÄìInference Divide</strong> ‚Üí During training, MDLMs are trained to predict all randomly masked tokens in a single pass, while at inference, MDLMs follow a model-dependent, confidence-guided unmasking schedule that progressively reveals structure of the generated sequence.</li></ul><ul id="253f34ea-c0ab-8040-a2c5-c10725479e48" class="bulleted-list"><li style="list-style-type:disc"><strong>Rigid Remasking ‚Üí </strong>For generating a sequence of tokens, MDLMs alternate between predicting all masked tokens and selectively remasking a fraction of the predictions based on heuristics and uncalibrated token-wise scores. The common remasking approaches freeze tokens once predicted, making it impossible to revise early low-confidence predictions later.  </li></ul><p id="253f34ea-c0ab-8005-8ae8-e1b695f82429" class="">These two problems are largely overlooked by previous works however hinder MDLMs to yield effective denoising trajectories. In this work, we address both problems with two complementary methods. Our contributions are summarized as below:</p><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="1edf34ea-c0ab-817a-9eff-e04915eabc4d"><div style="font-size:1.5em"><span class="icon">üí°</span></div><div style="width:100%"><ol type="1" id="1edf34ea-c0ab-81eb-94b1-ca8761a51409" class="numbered-list" start="1"><li><mark class="highlight-red">We observe a novel and previously unknown phenomenon in MDLMs: models occasionally produce correct answers at intermediate steps but `refine&#x27; them into incorrect results. We refer to this phenomenon as Over-denoising, which inspires us to supervise models not only on the final results but also intermediate denoising steps. </mark></li></ol><ol type="1" id="1edf34ea-c0ab-81fc-b9f7-e551cdd0d577" class="numbered-list" start="2"><li><mark class="highlight-red">We propose Masked Diffusion Policy Optimization (MDPO) to learn effective denoising trajectories without direct supervision. By exploiting the fact that MDLMs yield complete generations at every inference step, MDPO optimizes the model with policy gradients via intermediate-step rewards. Unlike prior RL fine-tuning on MDLMs, MDPO explicitly targets addressing the training-inference divide overlooked by previous works.</mark></li></ol><ol type="1" id="1edf34ea-c0ab-81c9-a3c7-c0b49113e3ac" class="numbered-list" start="3"><li><mark class="highlight-red">Instead of freezing tokens based on their single-step confidence, RCR allows flexible remasking by continuously tracking the running lowest confidence over denoising trajectories. Our experiments demonstrate consistently improved performance of using RCR on both LLaDA pre-trained as well as MDPO fine-tuned models.</mark></li></ol></div></figure><nav id="1edf34ea-c0ab-8154-93f5-e4a1928c3c09" class="block-color-gray table_of_contents"><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#1f3f34ea-c0ab-80a0-8136-fbdec3bd7253">1. Over-denoising</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#253f34ea-c0ab-8065-ac14-d224a400bea1">2. Training MDLMs as a Sequential Decision-making Problem</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#253f34ea-c0ab-8012-9356-f136c6ccff02">3. Flexible Remasking Enables MDLMs to Correct Early Mistakes</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#253f34ea-c0ab-8050-bd6c-f9678d6beae0">4. Results</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#253f34ea-c0ab-80e1-8642-da129bd8a10b">4.1 Over-denoising is An Effective Data Filter</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#253f34ea-c0ab-8027-bd40-de33a148b3f5">4.2 Impact of Sampling Settings on MDPO</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#1edf34ea-c0ab-810d-b836-e2558ae1d72a">Citation</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#253f34ea-c0ab-80f6-9426-f02452930cd0"><strong>Acknowledgements</strong></a></div></nav><h1 id="1f3f34ea-c0ab-80a0-8136-fbdec3bd7253" class="">1. Over-denoising</h1><p id="253f34ea-c0ab-8087-a7eb-ee78c894cdec" class="">One key difference between MDLMs and AR models is that MDLMs yield all tokens at each step while ARs yiled a token each step. This key paradigm difference drives us to look at the model outputs at intermediate steps in our early stage experiments. Surprisingly, we found a very interesting phenomenon</p><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="1f3f34ea-c0ab-80bb-893e-cc558d5267ce"><div style="font-size:1.5em"><span class="icon">üò≤</span></div><div style="width:100%"><p id="1f3f34ea-c0ab-8014-8b6d-e99b60fe56e0" class=""><strong>Finding: LLaDA answers correctly at intermediate steps, while further denoising reverse correct to wrong answers</strong>. </p></div></figure><p id="253f34ea-c0ab-80eb-97bb-e599502792d0" class="">Note that inference with MDLMs (e.g., LLaDA) alternates between predicting all masked tokens and selectively re-masking a fraction of the prediction for iterative refinement. This involves multiple iterative denoising steps informed by the model&#x27;s confidence score, forming a trajectory with fewer and fewer masked tokens to be predicted, and more and more structure being revealed. <br>Below is a live demo of how the model (LLaDA) evolves the answer with 64 denoising steps. The model is prompted with: </p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="253f34ea-c0ab-8048-a1f4-e39e3c5f8dfe" class="code code-wrap"><code class="language-JavaScript" style="white-space:pre-wrap;word-break:break-all">What is the last nonzero digit to the right of the decimal point in the decimal expansion of $\frac{137}{500}$?
Let&#x27;s think step by step and output the final answer within \boxed{}.</code></pre><p id="253f34ea-c0ab-80dd-b9ec-d059e0b82733" class="">which is a mathematical task and the model is asked to put the answer in a pre-defined structure ‚Äú\boxed{}‚Äù. Ground truth answer should be ‚Äú\boxed{4}‚Äù.</p><figure id="1f3f34ea-c0ab-80e0-b6ba-fba67037e564"><div class="source">
	<iframe
    id="demo-frame"
    sandbox="allow-scripts allow-same-origin"
    src=""
  >
    Sorry, we couldn‚Äôt load the demo. Check your browser console for details.
  </iframe>
  <!-- Dropdown for predefined IDs -->
  <label for="model-select">Choose a Model:</label>
  <select id="model-select">
    <option value="">‚Äî select an ID ‚Äî</option>
    <option value="64_512_number_theory_255">LLaDA-64st-ID255</option>
    <!-- add more options here as needed -->
  </select>

  <button id="load-btn">Load Demo</button>
  <script>
    // Map of short IDs ‚Üí hosted demo URLs
    const demoMap = {
      '64_512_number_theory_255': './LLaDA-8B-Instruct_prompt_step_by_step_linear_64_512_test_number_theory_255_overtime_conf_False_continue_rephrase_False.html',
      // ‚Ä¶extend as you add more demos‚Ä¶
    };

    document.getElementById('load-btn').addEventListener('click', () => {
      const select = document.getElementById('model-select');
      let url = '';

      // Priority 1: if a dropdown option is chosen, use its mapped URL
      if (select.value && demoMap[select.value]) {
        url = demoMap[select.value];
      }

      if (!url) {
        return alert(
          '‚ùå Please either select a model ID from the dropdown or enter a valid HTTPS URL.'
        );
      }

      const iframe = document.getElementById('demo-frame');
      iframe.removeAttribute('srcdoc');
      iframe.src = url;
      iframe.style.display = 'block';
    });

    // Optional: clear URL input when selecting from dropdown
    document.getElementById('model-select').addEventListener('change', () => {
      document.getElementById('url-input').value = '';
    });
  </script>
</div></figure><p id="253f34ea-c0ab-80d6-a796-db6ef011ac3e" class="">By dragging the step bar, you will see how the predictions of each step change. Clicking on the ‚Äúprev‚Äù or ‚Äúnext‚Äù will also show you which tokens are remasked everytime. We use a <a href="https://github.com/huggingface/Math-Verify">math verifier </a>to detect if the model yields correct answer, for all intermediate steps and the final step. As shown in the demo, in the beginning of the denoising, correct answer appears a few times, and then the answer tokens are remasked and lost in further denoising. We refer to this phenomenon as Over-denoising</p><p id="1f3f34ea-c0ab-80f6-9a45-dec68a98d6b8" class="">We further analyse the Over-denoising phenomenon in more detail below</p><figure id="253f34ea-c0ab-80c4-8db6-ee538bcde60e" class="image"><a href="Mind%20The%20Gap!%20Overcoming%20The%20Training-inference%20Di%201edf34eac0ab8009945dd2775a19c4ac/over-denoising.jpg"><img style="width:710px" src="Mind%20The%20Gap!%20Overcoming%20The%20Training-inference%20Di%201edf34eac0ab8009945dd2775a19c4ac/over-denoising.jpg"/></a><figcaption>Analysis of samples where Over-denoising occur. (Left) Maximum length of consecutive correct steps (span) in the denoising trajectory. (Middle) Number of separate correct spans in a trajectory. (Right) Heatmap of the relative position of correct steps across the denoising process.</figcaption></figure><p id="253f34ea-c0ab-8021-9a23-ffdbb7bfd5f8" class="">The left and middle subfigures show that correct spans are typically short and fragmented, with trajectories often containing multiple separate correct spans instead of steadily accumulating correct steps. Such fragmentation increases the likelihood of losing correct tokens before reaching the final step. In addition, the heatmap shows that correct answers often appear surprisingly very early but tend to decay over subsequent steps instead of steadily accumulating. </p><p id="253f34ea-c0ab-8042-be08-e2def81a67fe" class="">These findings not only underline the necessity to address the training-inference divide for MDLMs, but also reveal that Over-denoising provides highly informative signals of intermediate steps for improving MDLMs, which inspires us to introduce a policy gradient methods using intermediate-step rewards to optimize MDLMs to yield effective denoising trajectories.</p><h1 id="253f34ea-c0ab-8065-ac14-d224a400bea1" class="">2. Training MDLMs as a Sequential Decision-making Problem</h1><p id="253f34ea-c0ab-8017-b72e-e6b532d8ef94" class="">A simple solution to address the training-inference discrepancy would be to fine-tune the model with ground-truth trajectories. However, such trajectories are inherently unavailable, as human-generated data does not capture iterative denoising paths.</p><p id="253f34ea-c0ab-8008-a7e0-d29167407acf" class="">Based on the Markov property diffusion poessesses, we propose to frame the problem of learning effective denoising trajectories as a sequential decision-making problem and use the resulting framework to apply reinforcement learning. Inspired by the Over-denoising, the intermediate steps can as well provide massive informative signals for updating the model. We therefore propose Masked Diffusion Policy Optimization (MDPO) to explicitly train the mask prediction network as an RL policy. Specifically, rollouts are created by sampling answers with the policy given a prompt, and then a reward model (e.g., math verifier) is used to evaluate the completions of intermediate and final steps. These measured rewards are then used to estimate the advantage of each step for updating the policy.</p><p id="253f34ea-c0ab-80fa-9354-ec5a506cb298" class="">In addition, we leverage group-relative advantage estimation from GRPO. Demonstration of MDPO is as below:</p><figure id="253f34ea-c0ab-8073-a56e-df65b3ea4231" class="image"><a href="Mind%20The%20Gap!%20Overcoming%20The%20Training-inference%20Di%201edf34eac0ab8009945dd2775a19c4ac/MDPO.jpg"><img style="width:2582px" src="Mind%20The%20Gap!%20Overcoming%20The%20Training-inference%20Di%201edf34eac0ab8009945dd2775a19c4ac/MDPO.jpg"/></a><figcaption>MDPO generates a group of answers given a query for RL rollouts. Then all completions at intermediate and final steps are verified with a reward model. Based on verified rewards, MDPO estimates the advantage of step $t$ by considering rewards of the other steps in the current rollout and step $t$ from other rollouts in the group. These estimated advantages are used for policy optimization.</figcaption></figure><p id="253f34ea-c0ab-803e-9a65-fb2746aad589" class="">We observe that MDPO matches the performance of the previous state-of-the-art (SOTA) method with <strong>60√ó</strong> fewer gradient updates, while achieving average improvements of <strong>9.6%</strong> on MATH500 and <strong>54.2%</strong> on Countdown over SOTA when trained within the same number of weight updates.</p><h1 id="253f34ea-c0ab-8012-9356-f136c6ccff02" class="">3. Flexible Remasking Enables MDLMs to Correct Early Mistakes</h1><p id="253f34ea-c0ab-8054-bc84-ec6ba8ceac5a" class="">For remasking tokens at intermediate steps, LLaDA propose random remasking and low-confidence remasking. Empirically they find that the confidence-based remasking strategy works better. However, they have one crucial limitation: both remasking strategies (used in LLaDA) assign masking scores only to predicted tokens at current step. The unmasked tokens remain fixed until the end of the denoising process. We consider this a crucial limitation, as the predicted tokens, particularly in the early steps, tend to be highly noisy due to the limited structure revealed at that stage. Freezing these noisy tokens until the end of the denoising process makes it more difficult to produce high-quality generation in practice.</p><p id="253f34ea-c0ab-80c0-8643-f6eff4ec7375" class="">To address this, we makes a simple yet effective change to the Low-Confidence Remasking (LCR) to Running Confidence Remasking (RCR). Instead of deciding based solely on the confidence at the current step, we track for each position the highest confidence it has achieved so far for predicting masked tokens along the denoising process. At each step, we identify the specific amount of positions whose running maximum confidence is the lowest and remask tokens at these positions. See the figure below for the demonstration.</p><figure id="253f34ea-c0ab-8008-b2c5-ed03271f8a78" class="image"><a href="Mind%20The%20Gap!%20Overcoming%20The%20Training-inference%20Di%201edf34eac0ab8009945dd2775a19c4ac/inference.jpg"><img style="width:2662px" src="Mind%20The%20Gap!%20Overcoming%20The%20Training-inference%20Di%201edf34eac0ab8009945dd2775a19c4ac/inference.jpg"/></a><figcaption>Comparison between Low-Confidence Remasking (LCR) and our proposed Running Confidence Remasking (RCR) during iterative denoising. For each step, we show tokens that are \emph{not} remasked. LCR freezes low-confidence tokens once unmasked, preventing further refinement, which potentially accumulates early-stage noise. For example, the token `problem&#x27; predicted and frozen by LCR at step 1 is wrong but maintained until the end of the denoising, which leads to the final wrong answer. Whereas RCR tracks the running maximum confidence for each position, allowing persistently low-confidence tokens to be refined in later steps, leading to higher-quality completions.</figcaption></figure><p id="253f34ea-c0ab-80be-ab58-d120ebd02366" class="">Empirically, with more structure being revealed along with denoising, earlier steps often yield low-confidence predictions, whereas later steps tend to converge to higher confidence. Under the LCR strategy, early tokens are retained despite their relatively low confidence if they happen to fall within the top-n set to be `kept&#x27; at that step and can not be refined in future steps. In contrast, as the above figure shows, RCR allows such tokens to be remasked in later steps if tokens at other positions surpass them in running confidence, enabling the model to revise uncertain predictions before producing the final output.</p><h1 id="253f34ea-c0ab-8050-bd6c-f9678d6beae0" class="">4. Results</h1><p id="201f34ea-c0ab-807b-b1f6-d67ef667c86c" class="">We conduct our experiments on two reasoning tasks: <a href="https://huggingface.co/datasets/HuggingFaceH4/MATH-500">MATH-500</a> and <a href="https://huggingface.co/datasets/Jiayi-Pan/Countdown-Tasks-3to4">Countdown</a>. A key factor to affect the generation performance of MDLMs is whether denoising is performed over the full sequence at once or in blocks. Previous works apply a semi-autoregressive strategy where the sequence is divided into several blocks and generated from left to right. Within each block, tokens are denoised in parallel. We refer to this setting as semi-AR, and contrast it with the setting of denoising all tokens in the entire sequence simultaneously (pure-Diff).</p><figure id="253f34ea-c0ab-8029-8179-eb9aaf37c8ab" class="image"><a href="Mind%20The%20Gap!%20Overcoming%20The%20Training-inference%20Di%201edf34eac0ab8009945dd2775a19c4ac/comparison_table.png"><img style="width:709.9921875px" src="Mind%20The%20Gap!%20Overcoming%20The%20Training-inference%20Di%201edf34eac0ab8009945dd2775a19c4ac/comparison_table.png"/></a><figcaption>Model performance on Mathematics and CountdownBest and second-best methods in each setting are shaded. For each task we report results on semi-AR (Block Size=128) and pure-Diff (Block Size=512) given the generation length of 512. We also compare the performance across multiple choices of denoising steps.</figcaption></figure><p id="253f34ea-c0ab-800f-b424-c84aa880c2cd" class="">We show that across all configurations, both MDPO and RCR individually improve substantially upon the LLaDA initialization, with RCR often achieving performance comparable to MDPO despite requiring no additional training. We remark that RCR, as a training-free method, even outperforms the training baselines in most of the settings for the MATH task. Notably, combining MDPO with RCR consistently yields further gains over either method alone, achieving the best or second-best performance in nearly all settings, which demonstrates that MDPO and RCR are complementary. We further observe that the relative performance gains are more pronounced in settings with fewer inference steps, indicating improved sampling efficiency.</p><h2 id="253f34ea-c0ab-80e1-8642-da129bd8a10b" class="">4.1 Over-denoising is An Effective Data Filter</h2><p id="253f34ea-c0ab-80ba-8591-d611ad0f0639" class="">One surprising observation is that training the model on only Over-denoising samples with MDPO yields the best results. Specifically, we first use the initialized model to run inference on the whole dataset to identify Over-denoising samples and then train only on this subset, which constitutes roughly 10% of the original data. The comparison between MDPO-all-data and MDPO in the following figure shows that the model trained with only Over-denoising excels in most settings, highlighting the effectiveness of Over-denoising as a data filter for MDPO.</p><figure id="253f34ea-c0ab-8060-91b1-e07d288f8d5d" class="image"><a href="Mind%20The%20Gap!%20Overcoming%20The%20Training-inference%20Di%201edf34eac0ab8009945dd2775a19c4ac/comparison.png"><img style="width:710px" src="Mind%20The%20Gap!%20Overcoming%20The%20Training-inference%20Di%201edf34eac0ab8009945dd2775a19c4ac/comparison.png"/></a><figcaption>Comparison of MDPO variants. We report final accuracy and proportion of Over-denoising cases for all models. MDPO-all-data is trained on all data samples whereas MDPO is trained on only Over-denoising data, both with rollouts sampled from a mixture of semi-AR and pure-Diff. MDPO-pure-Diff and MDPO-semi-AR represent models that are trained on rollouts sampled from only pure-Diff, and semi-AR, respectively.</figcaption></figure><h2 id="253f34ea-c0ab-8027-bd40-de33a148b3f5" class="">4.2 Impact of Sampling Settings on MDPO</h2><p id="253f34ea-c0ab-80b2-bed4-de6d6456941e" class="">Another question we want to investigate is how different sampling settings during the rollout collection of RL affect the final performance. To investigate this, we compare MDPO variants trained on (i) pure-Diff rollouts only, (ii) semi-AR rollouts only, and (iii) an even mixture of both. The above figure shows that training on a single mode yields the largest improvement in that mode‚Äôs evaluation setting, but often at the cost of performance in the other mode. The mixture strategy used in our main MDPO setting achieves a more balanced performance, matching or exceeding the best single-mode results in several configurations. This suggests that mixed sampling allows the policy to learn denoising behaviors that generalize across all inference strategies.</p><h1 id="1edf34ea-c0ab-810d-b836-e2558ae1d72a" class="">Citation</h1><p id="1edf34ea-c0ab-8139-b308-eeb0cb9ae045" class="">If you find this blog or our codebase useful, please consider citing:</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="1edf34ea-c0ab-81de-a7de-e860e3a22cc1" class="code code-wrap"><code class="language-JSON" style="white-space:pre-wrap;word-break:break-all">@misc{He2025MDPO,
  title={MDPO: Overcoming the Training-Inference Divide of Masked Diffusion Language Models},
  author={Haoyu He and Katrin Renz and Yong Cao and Andreas Geiger},
  year={2025},
  eprint={2508.13148},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2508.13148},}
</code></pre><h1 id="253f34ea-c0ab-80f6-9426-f02452930cd0" class=""><strong>Acknowledgements</strong></h1><p id="253f34ea-c0ab-8086-8ddc-e16c9858b2f7" class="">We thank Zehao Yu, Markus Flicke, and Madhav Iyengar for fruitful discussions in the preparation of the draft. We also thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting K. Renz.</p></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>